---
layout: post
title: vLLM
date: 2024-12-17 14:59 +0800
categories: [系统, Tools]
tags: [Tools]
---

[vLLM](https://docs.vllm.ai/en/latest/)

kv management
continue batching
quantization: GPTQ, AWQ, INT4, INT8, FP8
Flash Attention
speculative decoding
chunked prefill

flexible:
high-throughput serving with various decoding algorithms, including parallel sampling, beam search...
tensor & pipeline parallelism support for distributed inference

docker run --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model mistralai/Mistral-7B-v0.1