---
layout: post
title: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
date: 2024-11-01 14:56 +0800
categories: [Paper, ML]
tags: [ML]
---
https://arxiv.org/abs/2104.04473

介绍了 Megatron-LM 如何结合张量、流水线和数据并行性在 GPU 集群上高效训练大规模语言模型，以下是详细总结：

## 研究背景和目标

*  大规模语言模型训练挑战:
    * GPU 内存容量有限，无法容纳大型模型。
    * 计算操作数量多，训练时间过长。

* 研究目标
    * 展示如何组合张量、流水线和数据并行性以在数千个 GPU 上进行训练。
    * 提出一种新的交错流水线调度策略，提高吞吐量并控制内存占用。

## 并行方法介绍
* 数据并行（Data Parallelism）
> 每个 worker 拥有完整模型副本，输入数据集分片，workers 定期聚合梯度。
* 流水线模型并行（Pipeline Model Parallelism）
> 模型层在多个设备上分片，批次分成微批次进行流水线执行。介绍了默认调度（GPipe）和交错阶段调度两种方式，并分析了其管道气泡大小和内存占用情况。交错阶段调度可减少管道气泡大小，但增加了通信量。
* 张量模型并行（Tensor Model Parallelism）
> 模型的单个层在多个设备上分区，以 Megatron 对 Transformer 层的分区策略为例进行介绍。

## 并行化配置的性能分析
* 符号定义: 定义了并行化维度、GPU 数量、全局批次大小、微批次大小等符号。
* 张量和流水线模型并行: 分析了两者对管道气泡大小和设备间通信量的影响，提出在使用-GPU 服务器时，张量模型并行度应使用到，然后使用流水线模型并行扩展到更大模型。
* 数据和模型并行: 分别考虑了流水线模型并行和数据并行、数据并行和张量模型并行之间的相互作用，提出了模型并行大小的选择建议以及数据并行可用于扩展训练到更多 GPU。
* 微批次大小: 微批次大小影响模型训练吞吐量，其最优值取决于模型的吞吐量和内存占用特性、流水线深度、数据并行大小和批次大小。
* 激活重计算（Activation Recomputation）: 是一种权衡计算操作和内存占用的技术，分析了激活检查点数量对内存占用的影响，提出了最优的检查点设置方法。
## 实现细节
* 通信优化:  利用张量模型并行和流水线模型并行减少跨节点通信开销，提出了散射 / 收集通信优化方法。
* 计算优化: 对计算图进行了三种模型特定的优化：改变数据布局、生成融合内核、创建自定义内核。
## 实验评估
* 实验设置
> 在 Selene 超级计算机上使用混合精度进行实验，报告了每个 GPU 的吞吐量以及聚合吞吐量。使用不同大小的 GPT 模型进行实验，模型参数、隐藏大小、注意力头数量和层数等可变。
* 结果分析
    * 端到端性能：随着模型大小增加，实现了超线性扩展，达到了较高的吞吐量，估计了不同模型的端到端训练时间，证明是实际可行的。
    * 与 ZeRO - 3 比较：在不同模型上与 ZeRO - 3 比较，PTD - P 在吞吐量上表现更优，尤其是在 GPU 数量增加时，由于跨节点通信更少，优势更明显。
    * 流水线并行：评估了流水线并行的弱缩放性能，以及交错调度和非交错调度的性能比较，交错调度在计算性能上更优，但随着批次大小增加，差距缩小。
    * 并行配置比较：展示了不同并行化维度组合的性能权衡，包括张量与流水线并行、流水线与数据并行、张量与数据并行之间的比较，以及微批次大小对性能的影响。
    * 激活重计算：分析了激活重计算对吞吐量的影响，在小批次大小下会降低吞吐量，但在大批次大小下可提高吞吐量。
    * 散射 - 收集优化：该优化可提高通信密集型调度的吞吐量，最多可达 11%。
    * 融合算子：算子融合可提高模型的吞吐量，不同模型上提升幅度不同。
    * 节点间通信带宽：利用高带宽通信链路实现了良好的性能，低带宽链路会影响缩放性能。
    * 检查点加载和保存：大规模模型的检查点加载和保存是重要考虑因素，给出了万亿参数模型的相关数据。
## 相关工作
介绍了其他大规模模型训练技术，包括流水线模型并行的不同实现方式、分片数据并行、自动分区以及高性能计算在模型训练中的应用，并与本文方法进行了比较。
## 讨论和结论
* 主要贡献
> 展示了 PTD - P 如何实现高聚合吞吐量，同时讨论了各种并行类型的权衡以及它们组合时的相互作用。提出的方法可在合理时间内完成大规模模型的端到端训练。
* 可扩展性
> 虽然本文以 GPU 为中心，但部分思想可应用于其他类型的加速器。


https://medium.com/data-scientists-playground/transformer-attention%E7%9F%A9%E9%99%A3%E9%81%8B%E7%AE%97%E7%AF%87-47491140abdf

https://blog.csdn.net/lansebingxuan/article/details/1333879320