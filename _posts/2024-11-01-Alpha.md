---
layout: post
title: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
date: 2024-11-01 14:53 +0800
categories: [Paper, ML]
tags: [ML]
---
https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf

## 研究背景和目标
* 深度学习模型训练挑战：随着深度学习模型规模增大，在分布式集群上训练需要大量工程投入，包括手动调整数据、算子和流水线并行化方法。自动并行化这些模型可加速研究和生产，但面临复杂的计划空间。
* 研究目标：提出一种新的并行化方法，自动生成涵盖所有数据、算子和流水线并行性的执行计划，以高效训练大规模深度学习模型。

## Alpa 的核心概念
* 分层并行空间
*   分层方式：将并行性分为算子内（intra - operator）和算子间（inter - operator）并行。算子内并行沿着张量轴划分算子并分配到设备；算子间并行将模型切成不同阶段并在不同设备集上执行。
*   优势：利用计算集群通信带宽的不对称性，分别将算子内和算子间并行映射到高带宽和低带宽连接的设备上；可将问题分解为两个可处理的子问题，接近最优地解决每个层级的并行执行计划。

* 编译过程
*   算子内编译（Intra - op Pass）：采用 SPMD 风格的算子内并行，通过整数线性规划（ILP）为计算图中的每个算子选择最优并行算法，以最小化执行时间。包括定义设备网格和张量布局，考虑布局转换成本，通过 ILP 求解最优并行计划，并应用后 ILP 通信优化。
*   算子间编译（Inter - op Pass）：将模型和设备集群切成阶段 - 网格对，目标是最小化端到端流水线执行延迟。采用动态规划（DP）算法，考虑不同的子网格形状和算子内并行计划，通过枚举和优化来分配阶段到网格，并调用算子内编译获取成本。
*   运行时编排（Runtime Orchestration）：在确定阶段、设备网格及其分配后，算子内编译为每个阶段 - 网格对生成并行可执行文件，自动插入集体通信原语处理网格内通信；算子间编译实现并行编排过程以处理阶段间的跨网格通信，生成跨网格重新分片的通信计划和静态指令。

## 实验评估
* 实验设置
*   实现细节：用约 16K 行 Python 和 6K 行 C++ 实现，使用 Jax 作为前端，XLA 作为后端，分布式运行时使用 Ray、XLA 和 NCCL。
*   测试模型和环境：在包含 8 个节点和 64 个 GPU 的 Amazon EC2 集群上评估，针对 GPT - 3、GShard MoE 和 Wide - ResNet 等模型，涵盖不同架构和参数规模。
* 结果分析
*   端到端性能
*       GPT - 3：与 Megatron - LM 相比，Alpa 自动生成执行计划，在某些设置下缩放性更好；Alpa - generated 计划与 Megatron - LM 的最佳计划相似，但在数据并行时会划分权重更新操作，性能略有提升。
*       GShard MoE：与 DeepSpeed 相比，Alpa 自动发现结合算子内和算子间并行的最佳执行计划，在多个节点上实现更好的缩放，速度提升明显。
*       Wide - ResNet：对于结构更异构的 Wide - ResNet，Alpa 仍能实现可扩展性能，而其他基线方法存在内存不足或无法有效扩展的问题。
*   消融研究
*       算子内并行：ILP - based 解决方案在处理算子内并行时优于其他方法，如 ZeRO 和启发式策略，能更好地优化通信开销。
*       算子间并行：DP 算法在算子聚类和分配上优于基于规则的算法，可根据通信成本和计算平衡聚类算子，在不同模型架构上表现不同。
*   编译时间：编译时间随模型大小和 GPU 数量线性增长，通过并行编译阶段和使用成本模型加速，对于大型模型和集群可接受。
*   跨网格重新分片：在 Wide - ResNet 上评估跨网格重新分片优化，启用该优化可将通信从慢连接转移到快连接，带来显著速度提升。

## 研究贡献和局限性
* 贡献
*   构建了分层的并行执行计划空间，结合算子内和算子间并行。
*   设计了可处理的优化算法，在每个层级推导接近最优的执行计划。
*   实现了 Alpa 编译器系统，包括编译过程和运行时架构，以及系统优化。
* 局限性
*   未对不同阶段间的通信成本建模。
*   算子间编译的微批次数量是超参数，未优化。
*   以静态线性方式处理流水线并行，未考虑动态调度。
*   未优化计算和通信重叠的最佳方案，只能处理静态计算图。

GShard MoE: [Scaling giant models with conditional computation and automatic
sharding](https://arxiv.org/abs/2006.16668)

Megatron-LM
Efficient large-scale language
model training on gpu clusters using megatron-lm.

https://arxiv.org/abs/2104.04473

Megatron-lm: Training multi-billion parameter language models using model parallelism
https://arxiv.org/abs/1909.08053


DeepSpeed
Deepspeed: System optimizations enable
training deep learning models with over 100 billion parameters
https://dl.acm.org/doi/abs/10.1145/3394486.3406703

WIDE-ResNet
Wide residual
networks.
https://arxiv.org/abs/1605.07146


Jax
https://github.com/jax-ml/jax


DP
ILP

SPMD-style
XLA
Optimizing compiler for machine learning
https://openxla.org/xla/tf2xla?hl=zh-cn


GSPMD
Gspmd: General and scalable parallelization for ml
computation graphs
https://arxiv.org/abs/2105.04663


NCCL
The nvidia collective communication library
https://developer.nvidia.com/nccl

PFLOPS,